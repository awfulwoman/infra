# ZFS Push Backup Implementation

**Session Overview**: Created a new push-based ZFS backup script to send datasets from the backup server to an offsite remote server (host-albion), enabling bidirectional backup capabilities.

## What Was Done

- **Created new push backup script** (`ansible/roles/server-zbackups-new/templates/zfs-push-backups.py`):
  - Based on existing `zfs-pull-backups.py` but reverses direction: local send, remote receive
  - Added `-w` flag to all `zfs send` commands to support raw/encrypted dataset transfers
  - Implemented `--strip-prefix` argument (defaults to `{{ zfsbackup_dataset }}`) to simplify remote dataset paths
  - Added `ensure_remote_parent_exists()` function that creates parent datasets on remote using `zfs create -p`
  - Remote path transformation: strips local backup prefix so `slowpool/encryptedbackups/host-storage/...` becomes `host-storage/...` on remote
  - Handles both initial sync (full + incremental) and subsequent syncs (incremental only) like the pull script

- **Updated ZFS delegation permissions** in `ansible/roles/server-zbackups-new/tasks/main.yaml`:
  - Added `send` permission to the delegation list for the backup user
  - Template deployment was already configured to deploy both pull and push scripts

- **Investigated remote permission requirements**:
  - Identified that source datasets need `send` and `hold` permissions (now configured)
  - Identified that remote destination needs `create`, `mount`, and `receive` permissions
  - Started but reverted changes to `client-zbackups` role - approach needs reconsideration

## Key Decisions

- **Path stripping for cleaner remote structure**: Rather than replicating the full local path (e.g., `slowpool/encryptedbackups/host-storage/tank/data`), the script strips the backup pool prefix to create a cleaner remote hierarchy (`host-storage/tank/data`)

- **Automatic parent dataset creation**: The script ensures all parent datasets exist on the remote before attempting to receive snapshots, preventing errors from missing intermediate datasets

- **Raw encrypted transfers**: Using `-w` flag ensures encrypted datasets are sent in their raw encrypted form, maintaining encryption at rest on the offsite server without needing the encryption key there

- **Deferred remote permission configuration**: Rather than immediately implementing remote permissions via the `client-zbackups` role, decided to pause and rethink the approach for managing ZFS receive permissions on the offsite server

## Files Changed

- `ansible/roles/server-zbackups-new/templates/zfs-push-backups.py` (new file, ~250 lines)
- `ansible/roles/server-zbackups-new/tasks/main.yaml` (added `send` to ZFS delegation)

## Current State

- Push script is functionally complete and ready for testing
- Local (backup server) has necessary `send` permissions configured
- Remote (host-albion) still needs ZFS receive permissions configured before script can be tested end-to-end
- Changes are uncommitted in git

## Next Steps

1. **Determine remote permission strategy**:
   - Decide how to manage ZFS permissions on host-albion (offsite server)
   - Consider whether to extend `client-zbackups` role or create a new role
   - Determine what user should have receive permissions on remote

2. **Configure remote permissions**:
   - Set up `create`, `mount`, `receive` permissions on host-albion for the receiving dataset
   - Ensure SSH key-based authentication is configured between backup server and host-albion

3. **Test push workflow**:
   - Run initial push to verify full + incremental sync works
   - Test subsequent runs to verify incremental-only syncs work
   - Verify encrypted datasets remain encrypted on remote
   - Test with multiple datasets to ensure path stripping works correctly

4. **Integration**:
   - Add push backups to scheduled cron jobs alongside pull backups
   - Consider timing: should push happen after pull, or separately?
   - Add monitoring/alerting for push backup failures

## Notes

- The push script mirrors the pull script's logic for determining initial vs. subsequent syncs
- Remote datasets will have a cleaner hierarchy since the backup pool prefix is stripped
- The `-w` (raw) flag is critical for maintaining encryption without exposing keys to the remote
- Both scripts now deployed by the same template task in the role, keeping them in sync
- This enables a multi-tier backup strategy: remote servers → backup server → offsite server

## Code Snippets

**Path stripping logic**:
```python
parser.add_argument('--strip-prefix', default='{{ zfsbackup_dataset }}',
                    help='Prefix to strip from local dataset path for remote path')

# Later in code:
remote_dataset = local_dataset.replace(strip_prefix + '/', '', 1)
```

**Raw encrypted send**:
```python
# All zfs send commands now use -w flag
subprocess.run(['zfs', 'send', '-w', f'{dataset}@{earliest}'], ...)
subprocess.run(['zfs', 'send', '-w', '-I', f'@{earliest}', f'{dataset}@{latest}'], ...)
```

**Ensuring remote parent exists**:
```python
def ensure_remote_parent_exists(remote_user, remote_host, remote_dataset):
    parent = remote_dataset.rsplit('/', 1)[0]
    ssh_cmd = ['ssh', f'{remote_user}@{remote_host}',
               'zfs', 'create', '-p', parent]
    subprocess.run(ssh_cmd, check=False)  # Ignore if already exists
```

## Additional Work

- **Created `suspend-backups.sh`** - A simple bash script in the repo root that uses `mosquitto_pub` to publish a "suspend" message to the `servers/host-backups` MQTT topic (host configured via `MQTT_HOST` env var)
- This script is used to suspend the backup server when done working on the server-zfsbackups role
