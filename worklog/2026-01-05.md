# Policy-Driven ZFS Snapshot System Implementation

**Session Overview**: Designed and implemented a complete policy-driven ZFS snapshot management system that replaces Sanoid with a custom Python-based solution using systemd timers and dataset importance levels.

## What Was Done

- **Created `system-zfs-policy` role** - Complete Ansible role for policy-driven snapshots:
  - **Policy system**: Four importance levels (none, low, high, critical) with different snapshot frequencies and retention periods
  - **Python scripts**: Two template-based scripts for snapshot management:
    - `zfs-snapshot.py.j2`: Creates snapshots tagged with interval (hourly/daily/monthly/yearly)
    - `zfs-prune.py.j2`: Prunes old snapshots based on retention policies
  - **Systemd integration**: 8 systemd units (4 service/timer pairs) for automated execution:
    - Hourly snapshots at :00
    - Daily snapshots at 00:10
    - Monthly snapshots on the 1st at 00:20
    - Yearly snapshots on Jan 1st at 00:30
    - Hourly pruning runs 15 minutes after snapshot creation
  - **Filter plugin enhancement**: Extended `zfs_datasets.py` with `zfs_datasets_with_importance` filter
  - **Comprehensive documentation**: Detailed README covering policy table, usage, monitoring

- **Removed Sanoid-based snapshot system**:
  - Deleted `sanoid.conf.j2` template from `backups-zfs-client` role
  - Removed Sanoid installation and configuration tasks
  - Transitioned to new policy-based approach with better control and visibility

- **Enhanced ZFS documentation** (`docs/ZFS.md`):
  - Added "Backup Destinations" column to snapshot policy table
  - Clarified relationship between snapshots and backups
  - Documented the new policy-driven system

- **Bug fixes during development**:
  1. **JSON boolean conversion issue**: Jinja2's `to_json` filter outputs lowercase `true`/`false` but Python expects `True`/`False` - fixed by using `json.loads()` to parse the JSON string
  2. **Root privilege check**: Added explicit privilege check with helpful error message instead of cryptic permission denied errors
  3. **Missing daily snapshots**: Added daily snapshot support (was documented but not initially implemented)

- **GitHub workflow**:
  - Created Issue #120: "Feature request for policy-driven ZFS snapshots"
  - Created PR #121: Complete implementation with all role files
  - Squash-merged PR to main branch

## Key Decisions

- **Policy-based over configuration-based**: Instead of configuring snapshot schedules per-dataset, the new system uses dataset properties (importance level) to determine behavior. This scales better and reduces configuration complexity.

- **Four importance tiers**: Established clear tiers matching data criticality:
  - **none**: No automatic snapshots (manual snapshots only)
  - **low**: Hourly snapshots with 7-day retention
  - **high**: Hourly + daily for 30 days, monthly for 12 months
  - **critical**: Full retention (hourly for 30 days, daily for 1 year, monthly for 10 years, yearly for 20 years)

- **Python over shell scripts**: Python provides better error handling, logging, and maintainability compared to bash scripts. JSON parsing and subprocess management are more robust.

- **Systemd timers over cron**: Systemd provides better logging (journalctl integration), dependency management, and monitoring capabilities compared to traditional cron jobs.

- **Staggered scheduling**: Snapshot timers run at different minutes past the hour to avoid resource contention:
  - Hourly: :00
  - Daily: :10
  - Monthly: :20
  - Yearly: :30
  - Prune runs 15 minutes after corresponding snapshot timer

- **Prune after snapshot**: Each snapshot timer has a corresponding prune timer that runs 15 minutes later to clean up old snapshots immediately after creating new ones.

- **Tag-based snapshot naming**: Snapshots named with clear prefixes (`hourly-`, `daily-`, `monthly-`, `yearly-`) make it easy to identify snapshot type and apply retention policies.

- **Importance as dataset property**: Using ZFS user properties (`importance`) keeps policy configuration with the dataset itself, making it portable and self-documenting.

## Files Changed

**New role created** (`ansible/roles/system-zfs-policy/`):
- `defaults/main.yaml` - Policy definitions and timer schedules (66 lines)
- `handlers/main.yaml` - systemd daemon-reload handler
- `tasks/main.yaml` - Role deployment logic (98 lines)
- `templates/zfs-snapshot.py.j2` - Snapshot creation script (92 lines)
- `templates/zfs-prune.py.j2` - Snapshot pruning script (89 lines)
- `templates/zfs-snapshot@.service` - Snapshot service unit
- `templates/zfs-snapshot-hourly.timer` - Hourly snapshot timer
- `templates/zfs-snapshot-daily.timer` - Daily snapshot timer
- `templates/zfs-snapshot-monthly.timer` - Monthly snapshot timer
- `templates/zfs-snapshot-yearly.timer` - Yearly snapshot timer
- `templates/zfs-prune-hourly.timer` - Hourly prune timer
- `templates/zfs-prune-daily.timer` - Daily prune timer
- `templates/zfs-prune-monthly.timer` - Monthly prune timer
- `templates/zfs-prune-yearly.timer` - Yearly prune timer
- `README.md` - Comprehensive documentation (146 lines)

**Modified files**:
- `/Users/charlie/Code/infra/ansible/plugins/filters/zfs_datasets.py` - Added `zfs_datasets_with_importance` filter
- `/Users/charlie/Code/infra/ansible/roles/backups-zfs-client/tasks/main.yaml` - Removed Sanoid tasks
- `/Users/charlie/Code/infra/ansible/roles/backups-zfs-client/templates/sanoid.conf.j2` - Deleted
- `/Users/charlie/Code/infra/docs/ZFS.md` - Added backup destinations to policy table

## Current State

- Complete policy-driven snapshot system ready for deployment
- All code merged to main branch via PR #121
- Sanoid dependency removed from infrastructure
- Documentation updated with new system details
- Filter plugin extended to support importance-based dataset queries
- Working directory clean (no uncommitted changes)

## Next Steps

1. **Deploy to test host**:
   - Add `system-zfs-policy` role to a test host's playbook
   - Set importance levels on test datasets
   - Run playbook to deploy timers and scripts
   - Verify systemd timers are active and scheduled correctly

2. **Test snapshot creation**:
   - Manually trigger snapshot timers: `systemctl start zfs-snapshot-hourly.timer`
   - Verify snapshots are created with correct naming: `zfs list -t snapshot`
   - Check snapshot tags match importance levels
   - Verify logs show successful execution: `journalctl -u zfs-snapshot@hourly.service`

3. **Test pruning logic**:
   - Create old snapshots manually to test retention
   - Trigger prune timers: `systemctl start zfs-prune-hourly.timer`
   - Verify old snapshots are removed according to policy
   - Check that recent snapshots are preserved

4. **Validate importance levels**:
   - Test all four importance levels (none, low, high, critical)
   - Verify "none" importance skips snapshots entirely
   - Verify each level creates correct snapshot types
   - Confirm retention periods match policy definitions

5. **Roll out to production hosts**:
   - Add role to host-storage, host-homeassistant, host-backups playbooks
   - Set appropriate importance levels on production datasets:
    - Critical: Photos, documents, git repositories
    - High: Media libraries, configuration data
    - Low: Temporary data, caches
    - None: Downloads, scratch space
   - Run playbooks to deploy
   - Monitor first 24-48 hours of automated snapshots

6. **Monitor and tune**:
   - Watch systemd timer execution in journalctl
   - Verify snapshot disk space usage is acceptable
   - Adjust retention policies if needed
   - Consider adding alerting for failed snapshot/prune operations

7. **Address documentation gaps** (identified by reviewer):
   - Update policy table in docs/ZFS.md (currently shows "Frequently" instead of "Daily")
   - Remove "as yet unbuilt" language from docs
   - Add documentation for filter plugins
   - Document backup scheduling relationship with snapshots
   - Add monitoring/alerting recommendations

## Notes

- **JSON boolean gotcha**: When templating Python code with Jinja2, using `{{ var | to_json }}` on Ansible booleans produces lowercase `true`/`false` (JSON standard) but Python expects capitalized `True`/`False`. The fix is to parse the JSON string in Python: `json.loads('{{ var | to_json }}')` which converts to proper Python booleans.

- **Importance property convention**: The role expects datasets to have a `importance` user property set via ZFS. This is set in host_vars as part of the declarative `zfs:` structure and managed by the `system-zfs` role.

- **Snapshot naming convention**: Snapshots follow the pattern `{interval}-{timestamp}` (e.g., `hourly-2026-01-05T14:00:00`, `daily-2026-01-05T00:10:00`). This makes it trivial to filter snapshots by interval type during pruning.

- **Systemd timer dependencies**: The role creates one parameterized service (`zfs-snapshot@.service`) that takes the interval as a parameter. Each timer (hourly, daily, monthly, yearly) triggers this service with its corresponding interval name.

- **Prune timer coordination**: Each prune timer is scheduled 15 minutes after its corresponding snapshot timer. This ensures snapshots are created before pruning runs, avoiding race conditions.

- **Filter plugin location**: Custom Ansible filters live in `ansible/plugins/filters/`. The `zfs_datasets.py` plugin provides several filters for working with the declarative `zfs:` structure including the new `zfs_datasets_with_importance` filter.

- **Sanoid migration**: The old Sanoid-based system required installing and configuring a third-party tool with its own configuration syntax. The new system uses only Python stdlib and ZFS commands, reducing external dependencies.

- **Retention policy design**: The retention periods are designed to provide good time-based recovery options:
  - Recent changes: Hourly snapshots (last 7-30 days)
  - Medium-term recovery: Daily snapshots (last 30 days - 1 year)
  - Long-term archival: Monthly and yearly snapshots (1-20 years)

- **Performance considerations**: All snapshot operations run with JSON output parsing (`zfs list -H -o name,user:importance -t filesystem -r {pool} -j`) for reliable parsing. The `-H` flag disables headers, `-r` enables recursion, and `-j` provides JSON output.

- **Root requirements**: Both scripts require root privileges to execute ZFS commands. The systemd services run as root, and manual execution will fail with a clear error if not run with sufficient privileges.

## Code Snippets

**Policy definition (defaults/main.yaml)**:
```yaml
zfs_snapshot_policies:
  none:
    intervals: []
  low:
    intervals:
      - hourly
    retention:
      hourly: 168  # 7 days
  high:
    intervals:
      - hourly
      - daily
    retention:
      hourly: 720    # 30 days
      daily: 30      # 30 days
      monthly: 12    # 12 months
  critical:
    intervals:
      - hourly
      - daily
      - monthly
      - yearly
    retention:
      hourly: 720     # 30 days
      daily: 365      # 1 year
      monthly: 120    # 10 years
      yearly: 240     # 20 years
```

**Snapshot creation (zfs-snapshot.py.j2)**:
```python
def create_snapshots(datasets, interval):
    """Create snapshots for datasets that support the given interval."""
    timestamp = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')
    snapshot_name = f"{interval}-{timestamp}"

    for dataset in datasets:
        importance = dataset['importance']
        policy = POLICIES.get(importance, {})

        if interval not in policy.get('intervals', []):
            continue

        full_snapshot = f"{dataset['name']}@{snapshot_name}"
        subprocess.run(['zfs', 'snapshot', full_snapshot], check=True)
```

**Pruning logic (zfs-prune.py.j2)**:
```python
def prune_snapshots(datasets, interval):
    """Prune old snapshots based on retention policy."""
    for dataset in datasets:
        importance = dataset['importance']
        policy = POLICIES.get(importance, {})
        retention = policy.get('retention', {}).get(interval)

        if retention is None:
            continue

        snapshots = get_snapshots(dataset['name'], interval)
        if len(snapshots) > retention:
            to_delete = snapshots[:-retention]  # Keep most recent N
            for snap in to_delete:
                subprocess.run(['zfs', 'destroy', snap], check=True)
```

**Filter plugin extension (zfs_datasets.py)**:
```python
def zfs_datasets_with_importance(zfs_config, importance_level):
    """Filter datasets by importance level."""
    all_datasets = zfs_datasets_with_config(zfs_config)
    return [
        ds for ds in all_datasets
        if ds.get('properties', {}).get('importance') == importance_level
    ]
```

**Systemd timer example (zfs-snapshot-hourly.timer)**:
```ini
[Unit]
Description=Hourly ZFS snapshot timer

[Timer]
OnCalendar=hourly
Persistent=true

[Install]
WantedBy=timers.target
```

**Importance property in host_vars**:
```yaml
zfs:
  pools:
    fastpool:
      datasets:
        photos:
          config:
            compression: lz4
            properties:
              importance: critical
        downloads:
          config:
            properties:
              importance: none
```
